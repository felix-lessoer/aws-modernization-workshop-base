[
{
	"uri": "/",
	"title": "Elastic on AWS Workshop",
	"tags": [],
	"description": "",
	"content": "Running more reliable apps in AWS leveraging the Elastic serverless forwarder via AWS SAR Elastic helps you to overcome data silos and transform data into actionable insights. By using the approach described in the workshop you will be able to run more reliable apps and detect issues as early as possible. By leveraging the Elastic serverless forwarder you save time for the setup and resources.\n"
},
{
	"uri": "/00_introduction.html",
	"title": "Introduction",
	"tags": [],
	"description": "",
	"content": "In this workshop you will learn how to use Elastic in your AWS environment. We will create the full environment for you including an example application. Your task will be to identify issues within this application by using Elastic, solving these issues and finally get an idea about the usage.\nWorkshop Agenda Deploy the Elastic environment from AWS Marketplace Familiarize yourself with the environment in AWS and in Elastic Identify root causes for application specific issues within the application Apply Machine Learning to the data in order to identify hidden patterns Learning Objectives Learn how to start quick and easy with the Elastic Stack in AWS Learn how to solve challenging issues within applications running on AWS Learn how to incorporate Machine Learning capabilities for your analysis (AIOps) Who should attend This workshop is targeted towards roles in the following areas:\nApplication Development: Develop software, apps and services, and are responsible for creating observable code, and helping debug and fix code-level issues in dev \u0026amp; prod.\nDevOps / SRE / Operations / Application Support: Responsible for maintaining service availability, performance, and reliability SLAs, and resolving issues when they occur\nBackground knowledge for the Workshop We start from scratch. So no prior knowledge needed to complete the workshop. However having some kind of Elastic knowledge helps to get it done more efficiently and learn some tips and tricks at the same time.\nExpected time We’ve planned this workshop to be finished within 1-2 hours. You can pause at any time and continue working. However, the trial period for your Elastic Cluster within the AWS Marketplace is 7 days. After that period, your trial will automatically transform into a paid subscription if you do not delete the environment.\n"
},
{
	"uri": "/00_introduction/page-1-elastic.html",
	"title": "What is Elastic?",
	"tags": [],
	"description": "",
	"content": "At Elastic, we see endless possibilities in a world of endless data. And we use the power of search to help people and organizations turn that possibility into results.\nAs the leading platform for search-powered solutions, we help everyone — organizations, their employees, and their customers — accelerate the results that matter.\nWith solutions in Enterprise Search, Observability, and Security, we help people find what they need faster, keep mission-critical applications running smoothly, and protect against cyber threats.\nWherever and however our customers put Elastic to work, we help them search, solve, and succeed — at scale and on a single platform. That’s the power of Elastic.\nDriving insights and action based on data is critical in order to fully benefit from the agility and flexibility enabled by cloud. With Elastic’s observability solution, you can unify visibility across your entire AWS and on premises environments, enabling better understanding of the availability, performance, and overall health of your infrastructure, applications, and business. AWS gives you a broad range of logs and metrics into their cloud services that allow you to monitor your cloud deployment and make more informed decisions. Elastic Observability integrates with these data sources to bring your data together in a unified manner, enabling you to continuously gain actionable insights into your IT, operations, and business. Easily analyze your data within prebuilt dashboards and tools or build custom visualizations that allow you to react quickly in regards to your business needs.\nUse Cases that can get achieved after finishing the workshop Cloud monitoring Elastic Observability allows your IT team to review, observe, and manage operational workflows in an automated way for insights across platforms and cloud environments.\nDevOps Enable your DevOps teams to improve how they bring apps to market by leveraging Elastic Observability across the entire app lifecycle.\nCloud migration Gain deep visibility into cloud and on-premises environments with Elastic Observability, leveraging insights on your infrastructure, migration dependencies, and more.\nCloud-native As cloud-native apps evolve, use Elastic Observability for visibility across the entire cloud-native stack, bringing logs, metrics, and traces together for better context.\nAIOps Elastic Observability can help AIOps teams use automation to simplify troubleshooting, speeding anomaly detection and problem resolution.\n"
},
{
	"uri": "/01_getting-started.html",
	"title": "Getting started",
	"tags": [],
	"description": "",
	"content": "Start the Elastic Environment via AWS Marketplace Follow the video below to see how to enable Elastic via AWS Marketplace as well as how to get the credentials for the Elastic environment. As we will use a cloud formation script in order to prepare your workshop environment including an example application - you need to run a different script then the one in the video. However this is still a good starting point if you like to test it also within your own AWS environment in a later stage.\n"
},
{
	"uri": "/01_getting-started/page-1-aws-marketplace.html",
	"title": "Enable Elastic in AWS Marketplace",
	"tags": [],
	"description": "",
	"content": "The first step you need to do is to activate Elastic in your AWS account. In order to do so the best way is to subscribe to Elastic via AWS Marketplace using ela.st/aws During the setup you need to create an Elastic Cloud Account which automatically starts your 7 days trial. You can skip step 3, which is installing the default Cloud Formation script. We will do that as workshop preparation in a later stage to also install the sample app for you. Finally click on \u0026ldquo;Launch product\u0026rdquo; which will lead you to the Elastic Cloud Configuration protal. "
},
{
	"uri": "/01_getting-started/page-1-elastic-key.html",
	"title": "Get credentials from Elastic",
	"tags": [],
	"description": "",
	"content": "In order to get credentials from the Elastic environment you need to create an API key within the Elastic Cloud Portal. The easiest way to do is to navigate to ela.st/cloud-key. From there just create the API and keep it save for later usage in your cloud formation script.\n"
},
{
	"uri": "/01_getting-started/page-1-cloud-formation.html",
	"title": "Run the Cloud Formation script",
	"tags": [],
	"description": "",
	"content": "Now we have everything in place in order to create your environment using Cloud Formation. Just click on the following button and enter the API key that you have created within your Elastic Cloud via AWS Marketplace environment (step 4)\nCreate your environment\rAdd your Elastic API Key to the template: Acknowledge the creation of resources: And finally click on create stack. Afterwards we will create the environment for you. This can take some time.\n"
},
{
	"uri": "/02_demo.html",
	"title": "Root Cause Analysis",
	"tags": [],
	"description": "",
	"content": "Monitoring the AWS environment is a very common use case for Elastic. You can download the complete ebook about that topic.\nElastic observes cross VPC, cross region, cross AZ, cross Account. No matter how you organize your environment, Elastic will fit into it. Collect everything in a single Elastic environment and authorize only the relevant team to it. Even if you are responsible to observe multiple different cloud providers or hybrid environments Elastic can help you getting better insights into what is happening in one single tool.\nElastic supports collecting data via different ways. Using the Elastic Agent, but also agentless and native integrations are available. In the introduction we already learned about the Elastic Agent and how this can get used. We will continue using the agent to observe the AWS platform and the apps you are running in it.\nHowever if you prefer not using the Elastic Agent you can collect the same data using the Elastic serverless forwarder which is a Lambda function that is able to collect the same data without the need to deploy an agent.\n"
},
{
	"uri": "/99_observability/page-1-platform.html",
	"title": "Observe the AWS platform",
	"tags": [],
	"description": "",
	"content": "Platform Logs In AWS you have three options to collect logs. This also holds for the platform / service specific logs. You can collect the data via the Serverless Log forwarder, via CloudWatch or collect the data from an S3 bucket.\nIf you decide for the S3 bucket you need to configure SQS notifications that get triggered whenever new data is written to the bucket. This approach is best practice to avoid significant lagging with polling. Elastic Agent combines notification and polling together by using Amazon SQS for Amazon S3 notification when a new Amazon S3 object is created.\nTo configure SQS Event Notifications click into the bucket that is holding your log data and navigate to “Properties”. Within the Properties section look for Event notifications and create a new entry.\nCloudtrail AWS CloudTrail enables governance, compliance, operational auditing, and risk auditing of your AWS account. This is helpful to observe the activities happening within your AWS environment(s). VPC Flow logs Elastic Observability allows you to quickly search, view, and filter Amazon Virtual Private Cloud (Amazon VPC) Flow Logs to monitor network traffic within your Amazon VPC with Kibana. With this integration, you can analyze the flow log data and compare it with your security group configurations to maintain and improve your cloud security. CloudWatch metrics With Elastic’s integrations and pre-built dashboards for AWS, you can collect AWS metrics such as usage, performance and more to see how every signal correlates — enabling you to make more informed business decisions. In order to collect AWS CloudWatch metrics you need to navigate to the installed integrations and change the AWS integration. Click on AWS → Integration policies → aws-1 (Default name) Just enable every integration that is delivering metrics. Screenshot is not showing all possible options. After enabling all the metrics you should be able to see the data in the out of the box dashboards.\nYou can also extend the out of the box dashboards by downloading pre-built dashboards from the Elastic Community.\nBilling data The AWS Billing data is a special part of the metric data from the previous chapter. The purpose of this data is to get mutual understanding of the current spending within the observed AWS accounts / organizations. Per default AWS only sends service level billing data which is nice to get a high level overview. However if your target is to reduce costs by correlating the usage data (logs and metrics) with the billing data this might be not enough details. AWS also offers to export more detailed billing data within the API or into a separate S3 bucket. Collecting this resource level billing data is very useful to compare the usage and the actual cost and therefore identify possible savings.\nThe AWS billing data is just another AWS integration that is available for the Elastic Agent. Just enable this integration and make sure that the access you are using has appropriate rights in order to get the data.\n"
},
{
	"uri": "/02_demo/page-1-environment.html",
	"title": "The environment",
	"tags": [],
	"description": "",
	"content": "Architecture We’ve prepared an AWS + Elastic environment for you. Within the AWS environment there is a simple python application deployed on EC2. This application is accessing DynamoDB and uses Lambda functions to enrich an object with more information. All the monitoring information including metrics and logs from the entire system are collected in Elastic. This is done by using one Elastic Agent at the EC2 instance and the Elastic Serverless forwarder for the logs. While the Elastic Agent would be also able to collect the different logs types this architecture has the advantage of running log monitoring on different hardware.\nThe architecture of the sample app looks like this While running the workshop you have access to your Elastic Cluster and to the EC2 instances that are running the application. In your Elastic Cluster you can find a dashboard called Workshop Dashboard. Use this dashboard as a starting point for all your tasks. You can solve all the tasks by using these components. You don’t need anything else. It\u0026rsquo;s expected that you find the issues using the Elastic Cluster and fix the issues within the EC2 environment(s).\nThe code that is running the application is downloaded from this GitHub Repo to the EC2 environment(s).\n"
},
{
	"uri": "/99_observability/page-1-example-app.html",
	"title": "Observe apps running in AWS",
	"tags": [],
	"description": "",
	"content": "While apps running in AWS use many of the available AWS services they all have their custom code as well. Consequently observing the AWS services and platform is only one part of the story. The other necessary part to get full visibility is to have the ability to look deep into what\u0026rsquo;s happening in your application as well by observing your individual logs, metrics and application traces.\nApplication logs In order to get your individual logs to Elastic you need to consider two things. First how to get the logs of all parts of the application in. If your application is using container or kubernetes technology this is quite simple.\nIf your application is running on EC2 instances you might need to collect custom log sources.\nIndependently of the log collection your also need to consider the log parsing. Per default custom logs will be collected as is. But most often the interesting information are within the log line itself. Therefore it can be helpful to parse the relevant information within the log lines and store them in separate fields. This makes it really handy to create visualizations and filters based on these extracted values.\nIf you are not sure whether you need something to parse out or not the best option is use a runtime field. A runtime field can be generated after the data is loaded and will extract the necessary information from any document that exists. However this comes with some performance drawbacks. If you know which kind of information you would like to extract the better option is to use pre index parsing using ingest node pipelines. Within an ingest node pipeline you can configure so-called processors to extract and fine tune the parsing before the data gets indexed to Elastic. The big advantage is that this will happen only once per document. In comparison to the runtime fields where the extraction happens on every query for every affected document.\nHaving a look into the integrations page within Elastic there are multiple possibilities to get your custom logs in. Elastic offers Serverless log forwarding using Lambda functions, as well as Log collection from S3, CloudWatch and directly from the EC2 machine. In addition to that you also could use the diverse language clients to directly send the logs to Elastic from your application.\nAfter you added the integration based on your need you can change the parsing like described above using the Ingest Pipelines (Under Stack Management).\nAPM (Traces) The collection and analysis of Application transactions and traces help you to get a deep understanding of what is happening within your application and how the different parts communicate to each other.\nUsing distributed tracing you also can have a look into the service map that will be created automatically based on the data and the service map also incorporates Elastic Machine Learning results. Lambda functions Many of the modern application within the AWS ecosystem also using Lambda function. With Elastic APM its also possible to collect the traces from the Lamdba functions in additions to the CloudWatch metrics that you can collect. The following pictures illustrates how Elastic APM integrates into your Lambda processes. "
},
{
	"uri": "/99_observability/page-1-k8s.html",
	"title": "Observe apps running in Kubernetes (EKS)",
	"tags": [],
	"description": "",
	"content": "Observe apps running in Kubernetes (EKS) This guide will cover how to use the Elastic Stack to provide comprehensive monitoring of your EKS Kubernetes environment including:\nContainer Logging Container Metrics Kubernetes posture management Kubernetes process and session monitoring EKS Audit Logging Kubernetes SIEM rules To be able to tell the Elastic Agent what data to collect we will create a policy with the relevant kubernetes integrations.\nIn the left navigation menu, select “Fleet” Select the “Agent policies” tab Select “Create agent policy” Call the policy something like “k8s” and leave the default to gather system logs and metrics. Kubernetes Integration Let\u0026rsquo;s start adding integrations to our new policy.\nClick the policy name. Click “Add integration” In the search bar lets search for “kubernetes”. This should give us the “Kubernetes“ and “Kubernetes Security Posture Management” integrations Let\u0026rsquo;s start with the Kubernetes integration. Click the integration and then “Add integration”. Leave all of the defaults and click “Save and continue”. This will do two things, it will add the assets for this integration and also create an instance of this integration within your policy. It will prompt you to add agents but we want to add some extra integrations first so select “Add Elastic Agent later”. Kubernetes Security Posture Management integration Now let\u0026rsquo;s add the Kubernetes Security Posture Management integration. Click “Add integration” again then find and click on the integration. You will see from the documentation that to be able to check the EKS cluster the integration will need an IAM user and it provides the required permissions.\nIn a new tab go to your AWS Console, go to the IAM dashboard and select “Policies” and “Create policy” Select the “JSON” tab and paste the permissions from the documentation.\n{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Sid\u0026#34;: \u0026#34;VisualEditor0\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;ecr:GetRegistryPolicy\u0026#34;, \u0026#34;eks:ListTagsForResource\u0026#34;, \u0026#34;elasticloadbalancing:DescribeTags\u0026#34;, \u0026#34;ecr-public:DescribeRegistries\u0026#34;, \u0026#34;ecr:DescribeRegistry\u0026#34;, \u0026#34;elasticloadbalancing:DescribeLoadBalancerPolicyTypes\u0026#34;, \u0026#34;ecr:ListImages\u0026#34;, \u0026#34;ecr-public:GetRepositoryPolicy\u0026#34;, \u0026#34;elasticloadbalancing:DescribeLoadBalancerAttributes\u0026#34;, \u0026#34;elasticloadbalancing:DescribeLoadBalancers\u0026#34;, \u0026#34;ecr-public:DescribeRepositories\u0026#34;, \u0026#34;eks:DescribeNodegroup\u0026#34;, \u0026#34;ecr:DescribeImages\u0026#34;, \u0026#34;elasticloadbalancing:DescribeLoadBalancerPolicies\u0026#34;, \u0026#34;ecr:DescribeRepositories\u0026#34;, \u0026#34;eks:DescribeCluster\u0026#34;, \u0026#34;eks:ListClusters\u0026#34;, \u0026#34;elasticloadbalancing:DescribeInstanceHealth\u0026#34;, \u0026#34;ecr:GetRepositoryPolicy\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; } ] } Select “Next:Tags”, then “Review” give the policy a name of “elastic-kubernetes-posture” and click “Create policy”. Now we have a policy with the correct permissions, we can create the service account for the agent to access EKS. Go to “Users” and “Add users”. Let’s name it “elastic-kubernetes-posture-user” and select “Access key” as the credentials. In the permissions screen we can select “Attach existing policies directly” and find the policy we just created called “elastic-kubernetes-posture”. Then go through the “Tags” and “Review” screens before confirming with “Create user”. We now have our user. You should be immediately given access key credentials. Copy both the Access key ID and Secret access key and we can head back to Elastic.\nBack in the posture management integration, click “Add Kubernetes Security Posture Management”. In the configuration screen, in “Kubernetes deployment” select “Kubernetes Security Posture Management“. You can then add the credentials for the user we just created. Your screen should look like the below image with the correct credentials.\nSelect “Save and continue” then again pick to deploy agents later.\nEndpoint and Cloud Security Back to your policy, let’s add one final integration. Back in “Add integration” select “Endpoint and Cloud Security” and then “Add Endpoint and Cloud Security”. Let’s call the integration “k8s-endpoint”. Click “save and continue”, again, choose to deploy agents later.\nWe now have 3 integrations in our policy so it should look like this.\nWe would like to collect session information from kubernetes hosts so let\u0026rsquo;s add that. Click on the name of the Endpoint integration we just created “k8s-endpoint” and you will be taken to the extensive configuration options. Scroll to the bottom and turn on “Include session data”.\nDeploying Elastic Agent in EKS To enable the Endpoint and Cloud Security integration to capture the required information we can follow the steps outlined in the documentation here: Elastic Kubernetes Documentation\nThis provides an OOTB manifest to deploy Elastic Agent, with Endpoint added, as a Daemonset to our EKS cluster. Download this example manifest file here: Example manifest\nFill in the manifest’s FLEET_URL field with your Fleet server’s Host URL. To find it, go to Kibana → Management → Fleet → Settings. Fill in the manifest’s FLEET_ENROLLMENT_TOKEN field with a Fleet enrollment token. To find one, go to Kibana → Management → Fleet → Enrollment tokens. Make sure to use or generate a token which is associated with the k8s policy. We can now apply the updated manifest to our EKS cluster to deploy our Elastic Agent Daemonset. E.g. kubectl apply -f elastic-endpoint-security.yaml Everything that is within your Kubernetes cluster will be monitored using Elastic now.\n"
},
{
	"uri": "/99_observability/page-1-conpliance.html",
	"title": "Observe vulnerabilities and compliance",
	"tags": [],
	"description": "",
	"content": "Observing vulnerabilities and compliance Operating your services within AWS enables you to use the wide range of AWS specific offerings in your application. Because of the complexity and the many different configuration options as well as the shared responsibilities between AWS your company its also possible to operate with less secure setups.\nIn order to prevent that Elastic also offers to support you with observing the configurations of your used services. To observe and prevent vulnerabilities you can use the Elastic Endpoint Integration that is part of the Elastic agent and use this to prevent any kind of malware or ransomware attacks to your used EC2 instances. Like with all other integrations the activation only needs a few clicks and is done after a few minutes.\nTo also observe compliance and configuration issues Elastic also provides Cloud Posture management in order to execute automatic audits and benchmarks against your environment.\nObserve WAF of your application "
},
{
	"uri": "/03_task1.html",
	"title": "Find issues in applications running on AWS",
	"tags": [],
	"description": "",
	"content": "Task 1 In the first task you have to find the root cause of an issue within our sample application. You can see that there is an issue because of the existence of error messages.\nYou can find error messages within the logs and also by looking at the APM data. Both ways will lead to the expected results. Choose the one that is more preferably for you.\nYou know that you’ve fixed the issue when there is no new error message coming in. After Task 1 You now have seen how a dashboard could be used to dive into the data and perform a root cause analysis for some simple issues. Elastic offers different kind of visualizations. Another very handy way to visualize your data is using Canvas. In contrast to the dashboard you have used to solve task 1, the Canvas dashboard is more graphical and less interactive. However it also has some unique capabilties like attaching metric data next to your e.g. architecture diagrams. That can help a lot in understanding where the issue is coming from. We\u0026rsquo;ve prepared a Canvas Dashboard in your Environment that visualizes the different ways log data is flowing into your cluster. Find the Canvas board in your environment and double check that all the data flows are working fine.\n"
},
{
	"uri": "/99_observability.html",
	"title": "Observability",
	"tags": [],
	"description": "",
	"content": "Monitoring the AWS environment is a very common use case for Elastic. You can download the complete ebook about that topic.\nElastic observes cross VPC, cross region, cross AZ, cross Account. No matter how you organize your environment, Elastic will fit into it. Collect everything in a single Elastic environment and authorize only the relevant team to it. Even if you are responsible to observe multiple different cloud providers or hybrid environments Elastic can help you getting better insights into what is happening in one single tool.\nElastic supports collecting data via different ways. Using the Elastic Agent, but also agentless and native integrations are available. In the introduction we already learned about the Elastic Agent and how this can get used. We will continue using the agent to observe the AWS platform and the apps you are running in it.\nHowever if you prefer not using the Elastic Agent you can collect the same data using the Elastic serverless forwarder which is a Lambda function that is able to collect the same data without the need to deploy an agent.\n"
},
{
	"uri": "/04_task2.html",
	"title": "Analyze performance issues",
	"tags": [],
	"description": "",
	"content": "Task 2 In the second task you have to find the reason for some bad performing transactions. The performance issue becomes visible after the first task is fixed. So it may take a bit of time until you can clearly see it in the charts. It\u0026rsquo;s also okay to reduce the time window of the Kibana charts to only get and see the data after the fix of task 1.\nYou know that you’ve fixed the issue when there is no transaction that takes longer than a couple of milliseconds.\nHint | Only use this if you have no idea how to proceed.\rIn order to find the issue you need to navigate to the APM Service of the Python App. The Python App only has one trace to analyze. Navigate into that trace and perform the Latency correlation. Make sure that you include enough data. The best would be to have 15 min of data after you fixed the issue in task 1. However if you are quicker you can use the time picker in Kibana to only use more recent data.\nWhen there is enough data you will find the concrete next step to do in order to fix the performance issue.\n"
},
{
	"uri": "/05_task3.html",
	"title": "AIOps Log Spike analysis",
	"tags": [],
	"description": "",
	"content": "Task 3 In the third task you have to find the reason for some anomalous log spikes. Every now and then the application is logging much more than usual. From a technical perspective there is no obvious reason for it, e.g. no cron job running.\nHint | Only use this if you have no idea how to proceed.\rWithin the Machine Learning menu in Kibana you can find \u0026ldquo;Explain Log Spike\u0026rdquo;. Use this with the Python Logs saved search.\nSolution | Open here if you think you found the issue and want to check your result.\r"
},
{
	"uri": "/06_task4.html",
	"title": "Anomaly Detection",
	"tags": [],
	"description": "",
	"content": "Task 4 In the third task you have to find the reason for some strange data within the last month.\nHint | Only use this if you have no idea how to proceed.\rWithin the Machine Learning menu in Kibana you can find \u0026ldquo;Explain Log Spike\u0026rdquo;. Use this with the Python Logs saved search.\n"
},
{
	"uri": "/08_conclusion.html",
	"title": "Conclusion",
	"tags": [],
	"description": "",
	"content": "After finishing the workshop your Elastic Environment is set up to collect and visualize all necessary data in order to observe everything that is happening within your infrastructure.\nTo finalize the setup it is necessary to include Elastic in your day to day processes. Best way to do this is adding alerting rules for your Machine Learning jobs and also for specific data / events that needs your attention.\nAfter setting up your set of alerting rules you might also want to automate the response to it. Every Elastic alert can have multiple connectors. That enables notifications as well as custom actions through e.g. webhooks.\n"
},
{
	"uri": "/categories.html",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/tags.html",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]