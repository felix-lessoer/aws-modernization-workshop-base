[
{
	"uri": "/",
	"title": "Elastic on AWS Workshop",
	"tags": [],
	"description": "",
	"content": "Elastic Observability on AWS Overcome data silos to transform your data into actionable insights\nWelcome In this workshop you will learn how to use Elastic in your AWS environment.\nWorkshop Agenda Deploy Elastic in AWS Marketplace Activate AWS data collection integrations Activate pre defined rules Build custom rules Build your first custom dashboard Learning Objectives Learn how to deploy Elastic in AWS Learn how to get any data from your AWS environments into Elastic Best practices for using the AWS data in regards to Observability, Security and Search Who should attend This workshop is targeted towards roles in the following areas:\nApplication Development: Develop software, apps and services, and are responsible for creating observable code, and helping debug and fix code-level issues in dev \u0026amp; prod.\nDevOps / SRE / Operations / Application Support: Responsible for maintaining service availability, performance, and reliability SLAs, and resolving issues when they occur\nBackground knowledge for the Workshop We start from scratch. So no prior knowledge needed to complete the workshop. However having some kind of Elastic knowledge helps to get it done more efficiently and learn some tips and tricks at the same time.\nExpected time We’ve planned this workshop to be finished within 1-2 hours. You can pause at any time and continue working. However, the trial period for your Elastic Cluster within the AWS Marketplace is 7 days. After that period, your trial will automatically transform into a paid subscription if you do not delete the environment.\n"
},
{
	"uri": "/00_introduction.html",
	"title": "Introduction",
	"tags": [],
	"description": "",
	"content": "What is Elastic? At Elastic, we see endless possibilities in a world of endless data. And we use the power of search to help people and organizations turn that possibility into results.\nAs the leading platform for search-powered solutions, we help everyone — organizations, their employees, and their customers — accelerate the results that matter.\nWith solutions in Enterprise Search, Observability, and Security, we help people find what they need faster, keep mission-critical applications running smoothly, and protect against cyber threats.\nWherever and however our customers put Elastic to work, we help them search, solve, and succeed — at scale and on a single platform. That’s the power of Elastic.\nDriving insights and action based on data is critical in order to fully benefit from the agility and flexibility enabled by cloud. With Elastic’s observability solution, you can unify visibility across your entire AWS and on premises environments, enabling better understanding of the availability, performance, and overall health of your infrastructure, applications, and business. AWS gives you a broad range of logs and metrics into their cloud services that allow you to monitor your cloud deployment and make more informed decisions. Elastic Observability integrates with these data sources to bring your data together in a unified manner, enabling you to continuously gain actionable insights into your IT, operations, and business. Easily analyze your data within prebuilt dashboards and tools or build custom visualizations that allow you to react quickly in regards to your business needs.\nUse Cases that can get achieved after finishing the workshop Cloud monitoring Elastic Observability allows your IT team to review, observe, and manage operational workflows in an automated way for insights across platforms and cloud environments.\nDevOps Enable your DevOps teams to improve how they bring apps to market by leveraging Elastic Observability across the entire app lifecycle.\nCloud migration Gain deep visibility into cloud and on-premises environments with Elastic Observability, leveraging insights on your infrastructure, migration dependencies, and more.\nCloud-native As cloud-native apps evolve, use Elastic Observability for visibility across the entire cloud-native stack, bringing logs, metrics, and traces together for better context.\nAIOps Elastic Observability can help AIOps teams use automation to simplify troubleshooting, speeding anomaly detection and problem resolution.\nHow to consume the workshop The workshop consists of three major parts:\nObserving the AWS platform Observing Apps running on AWS Observing vulnerabilities and compliance Each part can be done separately. However, if you want to learn how best to use Elastic (in order to consolidate your tool landscape, save costs, etc.), we recommend you complete the full workshop.\n"
},
{
	"uri": "/01_getting-started.html",
	"title": "Getting started",
	"tags": [],
	"description": "",
	"content": "Start the Elastic Environment via AWS Marketplace To get started, the first thing you need is your Elastic environment within AWS. While you have the option to deploy Elastic manually on top of EC2 instances, the easiest way to start is by using Elastic Cloud. You can start a seven-day free trial of Elastic Cloud via the subscription button on AWS Marketplace.\nIf you prefer to navigate to it manually, please follow these steps: Log in to the AWS Console Search for Elastic within the AWS Marketplace Choose Elastic Cloud (Elasticsearch Service)\nNow you need to click the “Click to subscribe” button and follow the sign up process within the Elastic Portal. Within the Elastic Portal you need to register a new account.\nNote: You need to use an email address that has not been used before. It\u0026rsquo;s not possible to convert existing accounts into AWS Marketplace accounts.\nUsing this newly created account you are able to create your first Elastic Cluster based on your needs.\nThe free trial includes provisioning of a single deployment and you will not be charged for the first 7 days (billing starts automatically after the 7-day trial period ends if you do not delete your deployment).\nWhen you subscribe to the Elasticsearch Service directly from the AWS Marketplace you then have the convenience of viewing your Elasticsearch Service subscription as part of your AWS bill, and you do not have to supply any additional credit card information to Elastic.\nMore details can be found in Elastic documentation.\nGet familiar with the environment When accessing Elastic Cloud for the first time, you will enter via the Elastic Cloud Portal. The Portal provides all necessary information and tools in order to manage your Elastic deployment(s) and everything associated (like receiving first class vendor support).\nElastic Cloud Portal Within the Elastic Cloud Portal you can create new Elastic environments. As Elastic is able to fulfill plenty of different use cases it might be useful to separate the different tasks by having one environment per use case. However for the purpose of this workshop, you can simply work with the one environment you get by using the trial.\nIn the top left panel you can see the overview of your current deployments within the Elasticsearch Service. Click on the name to get into the environment. When you click on the gear icon, you can get into the configuration options for the deployment. Here you can configure autoscaling, AWS Private Connect, IP filtering and other important security and size-related options.\nElastic Deployment When accessing the Elastic Deployment for the first time, you won’t have any data in it. Elastic offers you the option to load sample data. If you just want to look around, this is a quick and easy way to familiarize yourself with Elastic.\nHowever, in order to complete this workshop the best choice is to click on “Explore on my own” and navigate to “Add integrations”. But before we do this, let\u0026rsquo;s have a quick look at the rest of the navigation.\nAs described in the Introduction, Elastic offers 3 major Solutions: Enterprise Search, Observability and Security. The navigation reflects these solutions. In addition, you will also see the Analytics and Management section. Both sections are useful across all solutions.\nWithin the Analytics section, you have access to all the necessary tools to search and analyze your data. Machine Learning and custom dashboarding are also part of this section.\nThe Management section provides a UI to manage everything that belongs to the Elastic Stack. Here you will find data onboarding as well as alerts creation and security detail configurations.\nActivate the first AWS integration To add your first integration, you need to have an Elastic Agent up and running. The Elastic Agent can run on any EC2 instance or other VM. There are other options, like Lambda functions to load data as well.\nBefore we start adding the first AWS integration, we need to make sure that we have the right AWS Credentials and Permissions first.\nAWS Credentials AWS credentials are required to run AWS integrations. There are a few ways to provide AWS credentials:\nUse access keys directly Use temporary security credentials Use a shared credentials file Use an IAM role Amazon Resource Name (ARN) For this workshop, we just use the access keys directly. To do that, navigate to the IAM settings → Users → Choose the user you would like to use → Security credentials and create a new access key pair. AWS Permissions Specific AWS permissions are required for the IAM user to make specific AWS API calls. To enable the AWS integration to collect metrics and logs from all supported services, make sure these permissions are given:\nec2:DescribeInstances ec2:DescribeRegions cloudwatch:GetMetricData cloudwatch:ListMetrics iam:ListAccountAliases rds:DescribeDBInstances rds:ListTagsForResource s3:GetObject sns:ListTopics sqs:ChangeMessageVisibility sqs:DeleteMessage sqs:ListQueues sqs:ReceiveMessage sts:AssumeRole sts:GetCallerIdentity tag:GetResources By clicking on “Add integrations”, you get to the list of all available integrations. Click on “AWS\u0026rsquo;\u0026rsquo; in the left bar in order to see the list of all available services. Please choose Amazon EC2 to start. Just click on “Add Amazon EC2” in the overview page of the integration and configure the details as shown below.\nThe Elastic frontend will guide you through the process of installing an agent, adding the integration, and waiting for the first data to arrive.. ! Note that the integration will collect the data via AWS CloudWatch and is only able to get the e.g. the EC2 logs if the EC2 instance is configured to collect them in CloudWatch.\nIf you want to collect the System metrics and Logs directly this is possible by activating the system integration as well. The system integration is collecting all relevant metrics and logs from the system that is running the agent. If you like to collect those from all EC2 instances you need to run one agent per instance.\nCheck the out of the box assets that you got After the successful setup of the AWS EC2 integration you will be able to see the data of your environment in your dashboards. To test is navigate to dashboards and type “EC2” in the search bar. You will find a Dashboard called [Metrics AWS] EC2 Overview that will contain the data you are looking for. Working with AWS integrations is a great way to collect the monitoring data that is provided by AWS via CloudWatch or the logs that are stored in S3. If you want to have a deeper view into your EC2 instances you can extend the data that is collected by the agent by adding more integrations. Learn more about that in Observe the AWS platform environment\n[Optional] Security / Authentication tbd\n"
},
{
	"uri": "/02_observability.html",
	"title": "Observability",
	"tags": [],
	"description": "",
	"content": "Monitoring the AWS environment is a very common use case for Elastic. You can download the complete ebook about that topic.\nElastic observes cross VPC, cross region, cross AZ, cross Account. No matter how you organize your environment, Elastic will fit into it. Collect everything in a single Elastic environment and authorize only the relevant team to it. Even if you are responsible to observe multiple different cloud providers or hybrid environments Elastic can help you getting better insights into what is happening in one single tool.\nElastic supports collecting data via different ways. Using the Elastic Agent, but also agentless and native integrations are available. In the introduction we already learned about the Elastic Agent and how this can get used. We will continue using the agent to observe the AWS platform and the apps you are running in it.\nHowever if you prefer not using the Elastic Agent you can collect the same data using the Elastic serverless forwarder which is a Lambda function that is able to collect the same data without the need to deploy an agent.\nObserve the AWS platform Platform Logs In AWS you have three options to collect logs. This also holds for the platform / service specific logs. You can collect the data via the Serverless Log forwarder, via CloudWatch or collect the data from an S3 bucket.\nIf you decide for the S3 bucket you need to configure SQS notifications that get triggered whenever new data is written to the bucket. This approach is best practice to avoid significant lagging with polling. Elastic Agent combines notification and polling together by using Amazon SQS for Amazon S3 notification when a new Amazon S3 object is created.\nTo configure SQS Event Notifications click into the bucket that is holding your log data and navigate to “Properties”. Within the Properties section look for Event notifications and create a new entry.\nCloudtrail AWS CloudTrail enables governance, compliance, operational auditing, and risk auditing of your AWS account. This is helpful to observe the activities happening within your AWS environment(s). VPC Flow logs Elastic Observability allows you to quickly search, view, and filter Amazon Virtual Private Cloud (Amazon VPC) Flow Logs to monitor network traffic within your Amazon VPC with Kibana. With this integration, you can analyze the flow log data and compare it with your security group configurations to maintain and improve your cloud security. CloudWatch metrics With Elastic’s integrations and pre-built dashboards for AWS, you can collect AWS metrics such as usage, performance and more to see how every signal correlates — enabling you to make more informed business decisions. In order to collect AWS CloudWatch metrics you need to navigate to the installed integrations and change the AWS integration. Click on AWS → Integration policies → aws-1 (Default name) Just enable every integration that is delivering metrics. Screenshot is not showing all possible options. After enabling all the metrics you should be able to see the data in the out of the box dashboards.\nYou can also extend the out of the box dashboards by downloading pre-built dashboards from the Elastic Community.\nBilling data The AWS Billing data is a special part of the metric data from the previous chapter. The purpose of this data is to get mutual understanding of the current spending within the observed AWS accounts / organizations. Per default AWS only sends service level billing data which is nice to get a high level overview. However if your target is to reduce costs by correlating the usage data (logs and metrics) with the billing data this might be not enough details. AWS also offers to export more detailed billing data within the API or into a separate S3 bucket. Collecting this resource level billing data is very useful to compare the usage and the actual cost and therefore identify possible savings.\nThe AWS billing data is just another AWS integration that is available for the Elastic Agent. Just enable this integration and make sure that the access you are using has appropriate rights in order to get the data.\nObserve apps running in AWS While apps running in AWS use many of the available AWS services they all have their custom code as well. Consequently observing the AWS services and platform is only one part of the story. The other necessary part to get full visibility is to have the ability to look deep into what\u0026rsquo;s happening in your application as well by observing your individual logs, metrics and application traces.\nApplication logs In order to get your individual logs to Elastic you need to consider two things. First how to get the logs of all parts of the application in. If your application is using container or kubernetes technology this is quite simple.\nIf your application is running on EC2 instances you might need to collect custom log sources.\nIndependently of the log collection your also need to consider the log parsing. Per default custom logs will be collected as is. But most often the interesting information are within the log line itself. Therefore it can be helpful to parse the relevant information within the log lines and store them in separate fields. This makes it really handy to create visualizations and filters based on these extracted values.\nIf you are not sure whether you need something to parse out or not the best option is use a runtime field. A runtime field can be generated after the data is loaded and will extract the necessary information from any document that exists. However this comes with some performance drawbacks. If you know which kind of information you would like to extract the better option is to use pre index parsing using ingest node pipelines. Within an ingest node pipeline you can configure so-called processors to extract and fine tune the parsing before the data gets indexed to Elastic. The big advantage is that this will happen only once per document. In comparison to the runtime fields where the extraction happens on every query for every affected document.\nHaving a look into the integrations page within Elastic there are multiple possibilities to get your custom logs in. Elastic offers Serverless log forwarding using Lambda functions, as well as Log collection from S3, CloudWatch and directly from the EC2 machine. In addition to that you also could use the diverse language clients to directly send the logs to Elastic from your application.\nAfter you added the integration based on your need you can change the parsing like described above using the Ingest Pipelines (Under Stack Management).\nAPM (Traces) The collection and analysis of Application transactions and traces help you to get a deep understanding of what is happening within your application and how the different parts communicate to each other.\nUsing distributed tracing you also can have a look into the service map that will be created automatically based on the data and the service map also incorporates Elastic Machine Learning results. Lambda functions Many of the modern application within the AWS ecosystem also using Lambda function. With Elastic APM its also possible to collect the traces from the Lamdba functions in additions to the CloudWatch metrics that you can collect. The following pictures illustrates how Elastic APM integrates into your Lambda processes. Observing vulnerabilities and compliance Operating your services within AWS enables you to use the wide range of AWS specific offerings in your application. Because of the complexity and the many different configuration options as well as the shared responsibilities between AWS your company its also possible to operate with less secure setups.\nIn order to prevent that Elastic also offers to support you with observing the configurations of your used services. To observe and prevent vulnerabilities you can use the Elastic Endpoint Integration that is part of the Elastic agent and use this to prevent any kind of malware or ransomware attacks to your used EC2 instances. Like with all other integrations the activation only needs a few clicks and is done after a few minutes.\nTo also observe compliance and configuration issues Elastic also provides Cloud Posture management in order to execute automatic audits and benchmarks against your environment.\n"
},
{
	"uri": "/03_conclusion.html",
	"title": "Conclusion",
	"tags": [],
	"description": "",
	"content": "After finishing the workshop your Elastic Environment is set up to collect and visualize all necessary data in order to observe everything that is happening within your infrastructure.\nTo finalize the setup it is necessary to include Elastic in your day to day processes. Best way to do this is adding alerting rules for your Machine Learning jobs and also for specific data / events that needs your attention.\nAfter setting up your set of alerting rules you might also want to automate the response to it. Every Elastic alert can have multiple connectors. That enables notifications as well as custom actions through e.g. webhooks.\n"
},
{
	"uri": "/categories.html",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/tags.html",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]